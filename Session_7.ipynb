{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session-7",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn28QGZht2ua",
        "outputId": "748d5ca8-a529-4a6f-cd47-dc5e8f33f0e1"
      },
      "source": [
        "!pip install pytreebank\r\n",
        "\r\n",
        "\r\n",
        "!pip install google_trans_new\r\n",
        "\r\n",
        "import random\r\n",
        "import google_trans_new\r\n",
        "from google_trans_new import google_translator  \r\n",
        "# Import Library\r\n",
        "import random\r\n",
        "import torch, torchtext\r\n",
        "from torchtext import data\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import pytreebank"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytreebank in /usr/local/lib/python3.6/dist-packages (0.2.7)\n",
            "Requirement already satisfied: google_trans_new in /usr/local/lib/python3.6/dist-packages (1.1.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Zma93vazcs"
      },
      "source": [
        "\r\n",
        "dataset = pytreebank.load_sst(\"/content/sentimentData\")\r\n",
        "\r\n",
        "for category in [\"train\", \"test\", \"dev\"]:\r\n",
        "    with open(\"sst_{}.txt\".format(category), \"w\") as outfile:\r\n",
        "        for item in dataset[category]:\r\n",
        "            outfile.write(\r\n",
        "                \"{}\\t{}\\n\".format(\r\n",
        "                    item.to_labeled_lines()[0][0] + 1, item.to_labeled_lines()[0][1]\r\n",
        "                )\r\n",
        "            )"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ0ktCAxhIZe",
        "outputId": "0707e099-a775-4df4-fb70-cf2c2772cbd8"
      },
      "source": [
        "df_train = pd.read_csv(r\"/content/sst_train.txt\",sep=\"  \",header=None)\r\n",
        "df_test = pd.read_csv(r\"/content/sst_test.txt\",sep=\"  \",header=None)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4tprA7Abc09"
      },
      "source": [
        "df_train.columns  = ['text']\r\n",
        "df_test.columns  = ['text']  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ln5KSGVkL4m"
      },
      "source": [
        "df_train['label'] = df_train['text'].apply(lambda x : x.split(\"\\t\")[0])\r\n",
        "df_train['text'] = df_train['text'].apply(lambda x : x.split(\"\\t\")[1])\r\n",
        "\r\n",
        "df_test['label'] = df_test['text'].apply(lambda x : x.split(\"\\t\")[0])\r\n",
        "df_test['text'] = df_test['text'].apply(lambda x : x.split(\"\\t\")[1])\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "lvneM9hns6Cl",
        "outputId": "fabb6ccd-c2e2-4597-f913-e2b370515b77"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Singer/composer Bryan Adams contributes a slew...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You 'd think by now America would have had eno...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yet the act is still charming here .</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text label\n",
              "0  The Rock is destined to be the 21st Century 's...     4\n",
              "1  The gorgeously elaborate continuation of `` Th...     5\n",
              "2  Singer/composer Bryan Adams contributes a slew...     4\n",
              "3  You 'd think by now America would have had eno...     3\n",
              "4               Yet the act is still charming here .     4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIKpi-oztyqU"
      },
      "source": [
        "#### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmc1nWhht2bD"
      },
      "source": [
        "def random_deletion(words, p=0.5): \r\n",
        "    if len(words) == 1: # return if single word\r\n",
        "        return \" \".join([x for x in words])\r\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \r\n",
        "    if len(remaining) == 0: # if not left, sample a random word\r\n",
        "        return \" \".join([x for x in [random.choice(words)]])\r\n",
        "    else:\r\n",
        "        return \" \".join([x for x in remaining])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-zxrtFBt1vJ"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHu9RCQCvNG7"
      },
      "source": [
        "#### Random Swap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eYpoP1vt1PD"
      },
      "source": [
        "def random_swap(sentence, n=5): \r\n",
        "    length = range(len(sentence)) \r\n",
        "    for _ in range(n):\r\n",
        "        idx1, idx2 = random.sample(length, 2)\r\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \r\n",
        "    \r\n",
        "    sentence = \" \".join([x for x in sentence])\r\n",
        "    return sentence"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsN_EbBQwNyl"
      },
      "source": [
        "#### Transaltion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8m3ueQkvTGZ"
      },
      "source": [
        "### code for the transalation:::\r\n",
        "\r\n",
        "def translation(sentence):\r\n",
        "  translator = google_translator()\r\n",
        "\r\n",
        "  available_langs = list(google_trans_new.LANGUAGES.keys()) \r\n",
        "  trans_lang = random.choice(available_langs) \r\n",
        "  translations = translator.translate(sentence, trans_lang) \r\n",
        "  translations_en_random = translator.translate(translations,'en') \r\n",
        "\r\n",
        "  return translations_en_random\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAFUMWfUzzE1"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ygORTLcvTrP"
      },
      "source": [
        "## function to create only a threshold to pick up the data from the dataframe for the augmentaiton purposes:::::\r\n",
        "\r\n",
        "def data_augmentation(data ,swapping=True,deletion=True,transalation=True ,threshold=0.3):\r\n",
        "\r\n",
        "  tmp_df = data.sample(frac = threshold)\r\n",
        "\r\n",
        "  if(swapping):\r\n",
        "    tmp_swap = tmp_df.copy()\r\n",
        "    tmp_swap['text'] = tmp_swap['text'].apply(lambda x : random_swap(x.split()))\r\n",
        "\r\n",
        "    data = data.append(tmp_swap,ignore_index=True)\r\n",
        "\r\n",
        "  if(deletion):\r\n",
        "    tmp_del = tmp_df.copy()\r\n",
        "    tmp_del['text'] = tmp_del['text'].apply(lambda x : random_deletion(x.split()))\r\n",
        "\r\n",
        "    data = data.append(tmp_del,ignore_index=True)\r\n",
        "\r\n",
        "  if(transalation):\r\n",
        "\r\n",
        "    tmp_tran = tmp_df.copy()\r\n",
        "    tmp_tran['text'] = tmp_tran['text'].apply(lambda x : translation(x))\r\n",
        "\r\n",
        "    data = data.append(tmp_tran,ignore_index=True)\r\n",
        "\r\n",
        "  \r\n",
        "  return data\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9yZBM-tvT8w",
        "outputId": "62c75948-16b6-4c4d-82ad-84e3dcdc244f"
      },
      "source": [
        "augmented_data = data_augmentation(df_train,deletion=True,transalation=False)\r\n",
        "df_train.shape , augmented_data.shape\r\n",
        "#augmented_data = pd.read_csv(r\"augmented_data.csv\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((8544, 2), (13670, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNA0p4Jn537P"
      },
      "source": [
        "#augmented_data.to_csv(\"augmented_data.csv\",index=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96Gl9ea6cmVk"
      },
      "source": [
        "# Manual Seed\r\n",
        "SEED = 43\r\n",
        "torch.manual_seed(SEED)\r\n",
        "text = data.Field(sequential=True,tokenize = 'spacy', batch_first =True, include_lengths=True)\r\n",
        "label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyJ43Hu3kR6d"
      },
      "source": [
        "fields = [('text', text),('label',label)]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N8ZqL06kaZR"
      },
      "source": [
        "example = [data.Example.fromlist([augmented_data.text[i],augmented_data.label[i]], fields) for i in range(augmented_data.shape[0])] "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPNl3OjWoPpC"
      },
      "source": [
        "stanford_Data = data.Dataset(example, fields)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxk1NhbZkbZh",
        "outputId": "681fb03d-1bf1-411a-9523-e6aa1ebdbcf6"
      },
      "source": [
        "(train, valid) = stanford_Data.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))\r\n",
        "(len(train), len(valid))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11620, 2050)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp3qpRLmnoHn",
        "outputId": "a09de4de-90f8-428d-c439-0bd4effc6195"
      },
      "source": [
        "vars(train.examples[35])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': '3',\n",
              " 'text': ['Eckstraordinarily', 'lame', 'and', 'Severely', 'boring', '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xSt-iHXtI3H"
      },
      "source": [
        "text.build_vocab(train)\r\n",
        "label.build_vocab(train)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zf97yTVtKLr",
        "outputId": "7180e29f-7e2f-445e-acb5-fff14fe4c7b2"
      },
      "source": [
        "print('Size of input vocab : ', len(text.vocab))\r\n",
        "print('Size of label vocab : ', len(label.vocab))\r\n",
        "print('Top 10 words appreared repeatedly :', list(text.vocab.freqs.most_common(10)))\r\n",
        "print('Labels : ', label.vocab.stoi)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  16150\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 9920), (',', 8862), ('the', 7486), ('and', 5513), ('a', 5468), ('of', 5356), ('to', 3769), ('-', 3336), ('is', 3161), (\"'s\", 3127)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7f7fdb5d49d8>, {'4': 0, '2': 1, '3': 2, '5': 3, '1': 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bawjhjlhybeS"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid), batch_size = 32, \r\n",
        "                                                            sort_key = lambda x: len(x.text),\r\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hrwJNnB7rgT"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waVNmrCKoUoL"
      },
      "source": [
        "import os, pickle\r\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \r\n",
        "    pickle.dump(text.vocab.stoi, tokens)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1xM_e9u5Muv"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class classifier(nn.Module):\r\n",
        "    \r\n",
        "    # Define all the layers used in model\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\r\n",
        "        \r\n",
        "        super().__init__()          \r\n",
        "        \r\n",
        "        # Embedding layer\r\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        \r\n",
        "        # LSTM layer\r\n",
        "        self.encoder = nn.LSTM(embedding_dim, \r\n",
        "                           hidden_dim, \r\n",
        "                           num_layers=n_layers, \r\n",
        "                           dropout=dropout,\r\n",
        "                           batch_first=True)\r\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\r\n",
        "        # try bidirectional and compare their performances\r\n",
        "        \r\n",
        "        # Dense layer\r\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\r\n",
        "        \r\n",
        "    def forward(self, text, text_lengths):\r\n",
        "        \r\n",
        "        # text = [batch size, sent_length]\r\n",
        "        embedded = self.embedding(text)\r\n",
        "        # embedded = [batch size, sent_len, emb dim]\r\n",
        "      \r\n",
        "        # packed sequence\r\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\r\n",
        "        \r\n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\r\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\r\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\r\n",
        "    \r\n",
        "        # Hidden = [batch size, hid dim * num directions]\r\n",
        "        dense_outputs = self.fc(hidden)   \r\n",
        "        \r\n",
        "        # Final activation function softmax\r\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\r\n",
        "            \r\n",
        "        return output"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6As1kcC5OtC"
      },
      "source": [
        "# Define hyperparameters\r\n",
        "size_of_vocab = len(text.vocab)\r\n",
        "embedding_dim = 300\r\n",
        "num_hidden_nodes = 100\r\n",
        "num_output_nodes = 5\r\n",
        "num_layers = 3\r\n",
        "dropout = 0.2\r\n",
        "\r\n",
        "# Instantiate the model\r\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvdBuB8k5Q9O",
        "outputId": "fda4c4ef-3fab-4131-e10b-d8826bc8cd7c"
      },
      "source": [
        "print(model)\r\n",
        "\r\n",
        "#No. of trianable parameters\r\n",
        "def count_parameters(model):\r\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n",
        "    \r\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(16150, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=3, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=100, out_features=5, bias=True)\n",
            ")\n",
            "The model has 5,167,905 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulxsSwFO5Voq"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "# define optimizer and loss\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\r\n",
        "criterion = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "# define metric\r\n",
        "def binary_accuracy(preds, y):\r\n",
        "    #round predictions to the closest integer\r\n",
        "    \r\n",
        "    _, predictions = torch.max(preds, 1)\r\n",
        "    \r\n",
        "    correct = (predictions == y).float() \r\n",
        "    acc = correct.sum() / len(correct)\r\n",
        "    return acc\r\n",
        "    \r\n",
        "# push to cuda if available\r\n",
        "model = model.to(device)\r\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Qgx6p4W5Xzd"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\r\n",
        "    \r\n",
        "    # initialize every epoch \r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "    \r\n",
        "    # set the model in training phase\r\n",
        "    model.train()  \r\n",
        "    \r\n",
        "    for batch in iterator:\r\n",
        "        \r\n",
        "        # resets the gradients after every batch\r\n",
        "        optimizer.zero_grad()   \r\n",
        "        \r\n",
        "        # retrieve text and no. of words\r\n",
        "        text, text_lengths = batch.text   \r\n",
        "        \r\n",
        "        # convert to 1D tensor\r\n",
        "        predictions = model(text, text_lengths).squeeze()  \r\n",
        "        \r\n",
        "        # compute the loss\r\n",
        "        loss = criterion(predictions, batch.label)        \r\n",
        "        \r\n",
        "        # compute the binary accuracy\r\n",
        "        acc = binary_accuracy(predictions, batch.label)   \r\n",
        "        \r\n",
        "        # backpropage the loss and compute the gradients\r\n",
        "        loss.backward()       \r\n",
        "        \r\n",
        "        # update the weights\r\n",
        "        optimizer.step()      \r\n",
        "        \r\n",
        "        # loss and accuracy\r\n",
        "        epoch_loss += loss.item()  \r\n",
        "        epoch_acc += acc.item()    \r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg5LGYPI5aSJ"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\r\n",
        "    \r\n",
        "    # initialize every epoch\r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_acc = 0\r\n",
        "\r\n",
        "    # deactivating dropout layers\r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    # deactivates autograd\r\n",
        "    with torch.no_grad():\r\n",
        "    \r\n",
        "        for batch in iterator:\r\n",
        "        \r\n",
        "            # retrieve text and no. of words\r\n",
        "            text, text_lengths = batch.text\r\n",
        "            \r\n",
        "            # convert to 1d tensor\r\n",
        "            predictions = model(text, text_lengths).squeeze()\r\n",
        "            \r\n",
        "            # compute loss and accuracy\r\n",
        "            loss = criterion(predictions, batch.label)\r\n",
        "            acc = binary_accuracy(predictions, batch.label)\r\n",
        "            \r\n",
        "            # keep track of loss and accuracy\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            epoch_acc += acc.item()\r\n",
        "        \r\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQjibqWh5cZe",
        "outputId": "628b93b0-57c3-495b-94cc-92f8c0f3b24b"
      },
      "source": [
        "N_EPOCHS = 100\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "for epoch in range(N_EPOCHS):\r\n",
        "     \r\n",
        "    # train the model\r\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\r\n",
        "    \r\n",
        "    # evaluate the model\r\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    # save the best model\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
        "    \r\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\r\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.584 | Train Acc: 26.89%\n",
            "\t Val. Loss: 1.583 |  Val. Acc: 27.12% \n",
            "\n",
            "\tTrain Loss: 1.563 | Train Acc: 32.67%\n",
            "\t Val. Loss: 1.570 |  Val. Acc: 30.29% \n",
            "\n",
            "\tTrain Loss: 1.517 | Train Acc: 37.86%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 34.76% \n",
            "\n",
            "\tTrain Loss: 1.457 | Train Acc: 44.67%\n",
            "\t Val. Loss: 1.517 |  Val. Acc: 37.64% \n",
            "\n",
            "\tTrain Loss: 1.405 | Train Acc: 50.62%\n",
            "\t Val. Loss: 1.503 |  Val. Acc: 38.80% \n",
            "\n",
            "\tTrain Loss: 1.355 | Train Acc: 56.04%\n",
            "\t Val. Loss: 1.494 |  Val. Acc: 39.86% \n",
            "\n",
            "\tTrain Loss: 1.313 | Train Acc: 60.66%\n",
            "\t Val. Loss: 1.483 |  Val. Acc: 41.20% \n",
            "\n",
            "\tTrain Loss: 1.274 | Train Acc: 65.03%\n",
            "\t Val. Loss: 1.480 |  Val. Acc: 41.06% \n",
            "\n",
            "\tTrain Loss: 1.239 | Train Acc: 68.48%\n",
            "\t Val. Loss: 1.468 |  Val. Acc: 42.84% \n",
            "\n",
            "\tTrain Loss: 1.208 | Train Acc: 71.45%\n",
            "\t Val. Loss: 1.465 |  Val. Acc: 43.08% \n",
            "\n",
            "\tTrain Loss: 1.182 | Train Acc: 73.87%\n",
            "\t Val. Loss: 1.454 |  Val. Acc: 44.71% \n",
            "\n",
            "\tTrain Loss: 1.157 | Train Acc: 76.49%\n",
            "\t Val. Loss: 1.448 |  Val. Acc: 45.53% \n",
            "\n",
            "\tTrain Loss: 1.139 | Train Acc: 77.91%\n",
            "\t Val. Loss: 1.441 |  Val. Acc: 45.82% \n",
            "\n",
            "\tTrain Loss: 1.126 | Train Acc: 78.95%\n",
            "\t Val. Loss: 1.442 |  Val. Acc: 45.58% \n",
            "\n",
            "\tTrain Loss: 1.114 | Train Acc: 79.84%\n",
            "\t Val. Loss: 1.433 |  Val. Acc: 47.26% \n",
            "\n",
            "\tTrain Loss: 1.094 | Train Acc: 82.59%\n",
            "\t Val. Loss: 1.433 |  Val. Acc: 46.15% \n",
            "\n",
            "\tTrain Loss: 1.080 | Train Acc: 84.38%\n",
            "\t Val. Loss: 1.423 |  Val. Acc: 47.02% \n",
            "\n",
            "\tTrain Loss: 1.060 | Train Acc: 86.28%\n",
            "\t Val. Loss: 1.423 |  Val. Acc: 47.40% \n",
            "\n",
            "\tTrain Loss: 1.045 | Train Acc: 87.65%\n",
            "\t Val. Loss: 1.421 |  Val. Acc: 47.26% \n",
            "\n",
            "\tTrain Loss: 1.035 | Train Acc: 88.60%\n",
            "\t Val. Loss: 1.416 |  Val. Acc: 47.93% \n",
            "\n",
            "\tTrain Loss: 1.022 | Train Acc: 89.72%\n",
            "\t Val. Loss: 1.412 |  Val. Acc: 48.41% \n",
            "\n",
            "\tTrain Loss: 1.013 | Train Acc: 90.40%\n",
            "\t Val. Loss: 1.409 |  Val. Acc: 48.85% \n",
            "\n",
            "\tTrain Loss: 1.004 | Train Acc: 91.31%\n",
            "\t Val. Loss: 1.408 |  Val. Acc: 48.80% \n",
            "\n",
            "\tTrain Loss: 0.998 | Train Acc: 91.80%\n",
            "\t Val. Loss: 1.411 |  Val. Acc: 48.12% \n",
            "\n",
            "\tTrain Loss: 0.992 | Train Acc: 92.24%\n",
            "\t Val. Loss: 1.412 |  Val. Acc: 48.41% \n",
            "\n",
            "\tTrain Loss: 0.986 | Train Acc: 92.74%\n",
            "\t Val. Loss: 1.404 |  Val. Acc: 49.42% \n",
            "\n",
            "\tTrain Loss: 0.982 | Train Acc: 93.05%\n",
            "\t Val. Loss: 1.405 |  Val. Acc: 49.52% \n",
            "\n",
            "\tTrain Loss: 0.982 | Train Acc: 92.94%\n",
            "\t Val. Loss: 1.398 |  Val. Acc: 50.24% \n",
            "\n",
            "\tTrain Loss: 0.976 | Train Acc: 93.57%\n",
            "\t Val. Loss: 1.405 |  Val. Acc: 48.89% \n",
            "\n",
            "\tTrain Loss: 0.973 | Train Acc: 93.72%\n",
            "\t Val. Loss: 1.408 |  Val. Acc: 48.80% \n",
            "\n",
            "\tTrain Loss: 0.971 | Train Acc: 93.80%\n",
            "\t Val. Loss: 1.410 |  Val. Acc: 48.61% \n",
            "\n",
            "\tTrain Loss: 0.969 | Train Acc: 94.02%\n",
            "\t Val. Loss: 1.402 |  Val. Acc: 49.13% \n",
            "\n",
            "\tTrain Loss: 0.967 | Train Acc: 94.19%\n",
            "\t Val. Loss: 1.398 |  Val. Acc: 49.81% \n",
            "\n",
            "\tTrain Loss: 0.966 | Train Acc: 94.21%\n",
            "\t Val. Loss: 1.389 |  Val. Acc: 50.96% \n",
            "\n",
            "\tTrain Loss: 0.966 | Train Acc: 94.30%\n",
            "\t Val. Loss: 1.391 |  Val. Acc: 50.77% \n",
            "\n",
            "\tTrain Loss: 0.964 | Train Acc: 94.33%\n",
            "\t Val. Loss: 1.399 |  Val. Acc: 49.66% \n",
            "\n",
            "\tTrain Loss: 0.962 | Train Acc: 94.55%\n",
            "\t Val. Loss: 1.394 |  Val. Acc: 50.10% \n",
            "\n",
            "\tTrain Loss: 0.961 | Train Acc: 94.63%\n",
            "\t Val. Loss: 1.393 |  Val. Acc: 50.29% \n",
            "\n",
            "\tTrain Loss: 0.959 | Train Acc: 94.76%\n",
            "\t Val. Loss: 1.390 |  Val. Acc: 50.58% \n",
            "\n",
            "\tTrain Loss: 0.956 | Train Acc: 94.99%\n",
            "\t Val. Loss: 1.387 |  Val. Acc: 50.87% \n",
            "\n",
            "\tTrain Loss: 0.955 | Train Acc: 95.02%\n",
            "\t Val. Loss: 1.388 |  Val. Acc: 51.30% \n",
            "\n",
            "\tTrain Loss: 0.956 | Train Acc: 94.99%\n",
            "\t Val. Loss: 1.387 |  Val. Acc: 51.06% \n",
            "\n",
            "\tTrain Loss: 0.957 | Train Acc: 94.92%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 49.38% \n",
            "\n",
            "\tTrain Loss: 0.963 | Train Acc: 94.32%\n",
            "\t Val. Loss: 1.384 |  Val. Acc: 51.59% \n",
            "\n",
            "\tTrain Loss: 0.956 | Train Acc: 95.07%\n",
            "\t Val. Loss: 1.383 |  Val. Acc: 51.39% \n",
            "\n",
            "\tTrain Loss: 0.956 | Train Acc: 94.99%\n",
            "\t Val. Loss: 1.379 |  Val. Acc: 52.02% \n",
            "\n",
            "\tTrain Loss: 0.955 | Train Acc: 95.10%\n",
            "\t Val. Loss: 1.382 |  Val. Acc: 51.68% \n",
            "\n",
            "\tTrain Loss: 0.953 | Train Acc: 95.22%\n",
            "\t Val. Loss: 1.384 |  Val. Acc: 51.49% \n",
            "\n",
            "\tTrain Loss: 0.951 | Train Acc: 95.35%\n",
            "\t Val. Loss: 1.385 |  Val. Acc: 51.83% \n",
            "\n",
            "\tTrain Loss: 0.951 | Train Acc: 95.37%\n",
            "\t Val. Loss: 1.383 |  Val. Acc: 51.63% \n",
            "\n",
            "\tTrain Loss: 0.952 | Train Acc: 95.34%\n",
            "\t Val. Loss: 1.378 |  Val. Acc: 52.12% \n",
            "\n",
            "\tTrain Loss: 0.953 | Train Acc: 95.29%\n",
            "\t Val. Loss: 1.378 |  Val. Acc: 51.49% \n",
            "\n",
            "\tTrain Loss: 0.952 | Train Acc: 95.37%\n",
            "\t Val. Loss: 1.378 |  Val. Acc: 52.16% \n",
            "\n",
            "\tTrain Loss: 0.951 | Train Acc: 95.46%\n",
            "\t Val. Loss: 1.373 |  Val. Acc: 52.88% \n",
            "\n",
            "\tTrain Loss: 0.950 | Train Acc: 95.58%\n",
            "\t Val. Loss: 1.377 |  Val. Acc: 52.16% \n",
            "\n",
            "\tTrain Loss: 0.951 | Train Acc: 95.47%\n",
            "\t Val. Loss: 1.380 |  Val. Acc: 51.49% \n",
            "\n",
            "\tTrain Loss: 0.951 | Train Acc: 95.43%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 52.36% \n",
            "\n",
            "\tTrain Loss: 0.950 | Train Acc: 95.56%\n",
            "\t Val. Loss: 1.382 |  Val. Acc: 51.25% \n",
            "\n",
            "\tTrain Loss: 0.953 | Train Acc: 95.25%\n",
            "\t Val. Loss: 1.372 |  Val. Acc: 52.93% \n",
            "\n",
            "\tTrain Loss: 0.950 | Train Acc: 95.48%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 52.02% \n",
            "\n",
            "\tTrain Loss: 0.949 | Train Acc: 95.58%\n",
            "\t Val. Loss: 1.373 |  Val. Acc: 52.55% \n",
            "\n",
            "\tTrain Loss: 0.948 | Train Acc: 95.72%\n",
            "\t Val. Loss: 1.369 |  Val. Acc: 52.88% \n",
            "\n",
            "\tTrain Loss: 0.948 | Train Acc: 95.62%\n",
            "\t Val. Loss: 1.370 |  Val. Acc: 52.98% \n",
            "\n",
            "\tTrain Loss: 0.948 | Train Acc: 95.65%\n",
            "\t Val. Loss: 1.375 |  Val. Acc: 52.26% \n",
            "\n",
            "\tTrain Loss: 0.947 | Train Acc: 95.74%\n",
            "\t Val. Loss: 1.366 |  Val. Acc: 53.17% \n",
            "\n",
            "\tTrain Loss: 0.947 | Train Acc: 95.76%\n",
            "\t Val. Loss: 1.370 |  Val. Acc: 52.79% \n",
            "\n",
            "\tTrain Loss: 0.949 | Train Acc: 95.59%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 52.12% \n",
            "\n",
            "\tTrain Loss: 0.949 | Train Acc: 95.66%\n",
            "\t Val. Loss: 1.370 |  Val. Acc: 52.84% \n",
            "\n",
            "\tTrain Loss: 0.947 | Train Acc: 95.78%\n",
            "\t Val. Loss: 1.376 |  Val. Acc: 52.36% \n",
            "\n",
            "\tTrain Loss: 0.945 | Train Acc: 95.96%\n",
            "\t Val. Loss: 1.367 |  Val. Acc: 53.17% \n",
            "\n",
            "\tTrain Loss: 0.944 | Train Acc: 96.01%\n",
            "\t Val. Loss: 1.370 |  Val. Acc: 53.22% \n",
            "\n",
            "\tTrain Loss: 0.944 | Train Acc: 96.02%\n",
            "\t Val. Loss: 1.374 |  Val. Acc: 52.31% \n",
            "\n",
            "\tTrain Loss: 0.946 | Train Acc: 95.90%\n",
            "\t Val. Loss: 1.379 |  Val. Acc: 51.15% \n",
            "\n",
            "\tTrain Loss: 0.949 | Train Acc: 95.59%\n",
            "\t Val. Loss: 1.367 |  Val. Acc: 53.08% \n",
            "\n",
            "\tTrain Loss: 0.946 | Train Acc: 95.89%\n",
            "\t Val. Loss: 1.370 |  Val. Acc: 52.84% \n",
            "\n",
            "\tTrain Loss: 0.945 | Train Acc: 95.96%\n",
            "\t Val. Loss: 1.369 |  Val. Acc: 53.41% \n",
            "\n",
            "\tTrain Loss: 0.944 | Train Acc: 96.02%\n",
            "\t Val. Loss: 1.372 |  Val. Acc: 52.69% \n",
            "\n",
            "\tTrain Loss: 0.944 | Train Acc: 96.08%\n",
            "\t Val. Loss: 1.375 |  Val. Acc: 51.97% \n",
            "\n",
            "\tTrain Loss: 0.944 | Train Acc: 96.01%\n",
            "\t Val. Loss: 1.370 |  Val. Acc: 52.88% \n",
            "\n",
            "\tTrain Loss: 0.944 | Train Acc: 96.05%\n",
            "\t Val. Loss: 1.368 |  Val. Acc: 53.22% \n",
            "\n",
            "\tTrain Loss: 0.944 | Train Acc: 96.08%\n",
            "\t Val. Loss: 1.363 |  Val. Acc: 53.65% \n",
            "\n",
            "\tTrain Loss: 0.944 | Train Acc: 96.05%\n",
            "\t Val. Loss: 1.377 |  Val. Acc: 52.31% \n",
            "\n",
            "\tTrain Loss: 0.947 | Train Acc: 95.85%\n",
            "\t Val. Loss: 1.367 |  Val. Acc: 53.32% \n",
            "\n",
            "\tTrain Loss: 0.944 | Train Acc: 96.10%\n",
            "\t Val. Loss: 1.365 |  Val. Acc: 53.37% \n",
            "\n",
            "\tTrain Loss: 0.943 | Train Acc: 96.17%\n",
            "\t Val. Loss: 1.360 |  Val. Acc: 54.37% \n",
            "\n",
            "\tTrain Loss: 0.943 | Train Acc: 96.19%\n",
            "\t Val. Loss: 1.364 |  Val. Acc: 53.61% \n",
            "\n",
            "\tTrain Loss: 0.942 | Train Acc: 96.26%\n",
            "\t Val. Loss: 1.363 |  Val. Acc: 53.89% \n",
            "\n",
            "\tTrain Loss: 0.942 | Train Acc: 96.30%\n",
            "\t Val. Loss: 1.359 |  Val. Acc: 53.80% \n",
            "\n",
            "\tTrain Loss: 0.942 | Train Acc: 96.21%\n",
            "\t Val. Loss: 1.364 |  Val. Acc: 53.51% \n",
            "\n",
            "\tTrain Loss: 0.944 | Train Acc: 96.09%\n",
            "\t Val. Loss: 1.368 |  Val. Acc: 53.03% \n",
            "\n",
            "\tTrain Loss: 0.943 | Train Acc: 96.15%\n",
            "\t Val. Loss: 1.362 |  Val. Acc: 53.70% \n",
            "\n",
            "\tTrain Loss: 0.943 | Train Acc: 96.19%\n",
            "\t Val. Loss: 1.354 |  Val. Acc: 54.52% \n",
            "\n",
            "\tTrain Loss: 0.944 | Train Acc: 96.09%\n",
            "\t Val. Loss: 1.367 |  Val. Acc: 53.08% \n",
            "\n",
            "\tTrain Loss: 0.941 | Train Acc: 96.33%\n",
            "\t Val. Loss: 1.359 |  Val. Acc: 54.37% \n",
            "\n",
            "\tTrain Loss: 0.941 | Train Acc: 96.33%\n",
            "\t Val. Loss: 1.357 |  Val. Acc: 54.04% \n",
            "\n",
            "\tTrain Loss: 0.941 | Train Acc: 96.35%\n",
            "\t Val. Loss: 1.360 |  Val. Acc: 53.99% \n",
            "\n",
            "\tTrain Loss: 0.945 | Train Acc: 95.98%\n",
            "\t Val. Loss: 1.357 |  Val. Acc: 54.09% \n",
            "\n",
            "\tTrain Loss: 0.943 | Train Acc: 96.27%\n",
            "\t Val. Loss: 1.360 |  Val. Acc: 54.04% \n",
            "\n",
            "\tTrain Loss: 0.941 | Train Acc: 96.39%\n",
            "\t Val. Loss: 1.359 |  Val. Acc: 53.80% \n",
            "\n",
            "\tTrain Loss: 0.941 | Train Acc: 96.38%\n",
            "\t Val. Loss: 1.359 |  Val. Acc: 53.94% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0E9vcVM6cm6"
      },
      "source": [
        "CUDA_LAUNCH_BLOCKING=1"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PyRJ1F58tfy"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}